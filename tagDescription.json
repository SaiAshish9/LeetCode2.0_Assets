{
  "biconnected_component": [
    "Biconnected components in a network are essentially the edges that, if removed, would disconnect the network. These are crucial for maintaining connectivity within the network.",
    "The problem typically involves representing a network as a graph, where nodes represent entities (such as computers or routers) and edges represent connections between these entities. The task is to find all the critical connections in the network, i.e., the edges whose removal would increase the number of connected components in the graph.",
    "One common approach to solving this problem is by using Tarjan's algorithm for finding articulation points and bridges in an undirected graph. In this algorithm, you perform a depth-first search (DFS) on the graph, keeping track of certain properties of the nodes and edges encountered during the traversal.",
    "Here's a high-level overview of the algorithm:",
    "1. Perform a DFS traversal of the graph, starting from any node\n2. During the traversal, maintain a timestamp for each node indicating when it was visited.\n3. Keep track of the lowest timestamp reachable from each node in the DFS traversal, which helps in identifying back edges.\n4. If you encounter a back edge (an edge to an ancestor in the DFS tree), update the lowest reachable timestamp for the corresponding node.\n5. If the lowest reachable timestamp of a child node is greater than or equal to the timestamp of the current node, then the edge connecting them is a critical connection.",
    "By applying Tarjan's algorithm, you can identify all the critical connections in the network efficiently. This algorithm typically runs in linear time complexity relative to the size of the graph.",
    "Once you've identified the critical connections, you can return them as a list of edges."
  ],
  "rejection_sampling": [
    "Rejection Sampling typically involves generating random samples that meet specific criteria or fit a desired distribution.",
    "LeetCode Problem: Implement Rand10() Using Rand7() (LeetCode 470)",
    "Problem Statement:",
    "You have a function rand7() that generates a random integer from 1 to 7. You need to write a function rand10() that generates a random integer from 1 to 10 uniformly using rand7().",
    "Rejection Sampling Approach:",
    "The key idea behind using rejection sampling for this problem is to transform the output of rand7() to achieve a uniform distribution over a different range (1 to 10) by generating a larger set of candidate numbers and then rejecting those that do not fit the desired criteria.",
    "Here's a step-by-step explanation of how rejection sampling can be applied to this problem:",
    "Generate a Candidate from rand7():",
    "1. Use rand7() twice to generate a number in the range 1 to 49. This can be done by interpreting the results of two rand7() calls as digits in a base-7 number.\n2. Specifically, compute the candidate number as (rand7() - 1) * 7 + rand7(). This generates a uniform integer in the range 1 to 49 because rand7() produces each number 1 through 7 with equal probability.",
    "Reject or Accept the Candidate:",
    "1. Since 49 is not a multiple of 10, not all numbers from 1 to 49 map evenly to the range 1 to 10. To maintain uniformity, we only accept numbers in the range 1 to 40 because 40 is the largest multiple of 10 less than 49.\n2. If the generated number falls within this range (1 to 40), we map it to the range 1 to 10 using modulo operation and return the result.\n3. If the number is outside this range (41 to 49), we reject it and repeat the process.",
    "Implementation in Python:",
    "Hereâ€™s how the rejection sampling algorithm can be implemented:",
    "import random",
    "def rand7():\n\treturn random.randint(1, 7)",
    "def rand10():\n\twhile True:\n\t\tnum = (rand7() - 1) * 7 + rand7()   Generate a number in the range 1 to 49\n\t\tif num <= 40:   Accept only if the number is in the range 1 to 40\n\t\t\treturn 1 + (num - 1) % 10   Map to range 1 to 10",
    "print([rand10() for _ in range(10)])",
    "Detailed Explanation:",
    "Generate Candidate: (rand7() - 1) * 7 + rand7() produces a number from 1 to 49 because rand7() generates a number from 1 to 7, and multiplying by 7 shifts the range accordingly.",
    "Acceptance Condition: if num <= 40 ensures we only consider numbers that can be mapped uniformly to 1 to 10.",
    "Mapping: 1 + (num - 1) % 10 converts the accepted number to the desired range 1 to 10.",
    "By only accepting numbers from 1 to 40, we ensure that each number from 1 to 10 can be mapped equally using modulo operation. Numbers 41 to 49 are rejected to avoid bias.",
    "This example illustrates how rejection sampling can be used to transform a random number generator into a uniform random generator over a different range. The key steps are generating a candidate, checking if it falls within the acceptable range, and rejecting or accepting it based on that criterion. This ensures that the resulting distribution is uniform and meets the problem requirements."
  ],
  "radix_sort": [
    "For input list with negative integers, please refer to the 912. Sort an Array question",
    "Radix sort is a non-comparative integer sorting algorithm that sorts data with integer keys by processing individual digits. It is particularly effective when the range of possible key values is not too large and is well-suited for sorting large lists of integers. Radix sort can be performed using either the least significant digit (LSD) or the most significant digit (MSD) method.\n\nHere's an overview of both methods:\n\nRadix sort can be a suitable choice for problems involving sorting large lists of integers or strings where the maximum value or length is not excessively large.\n\nLSD Radix Sort (Least Significant Digit)\n\n1. Initialize:\n   - Start with the least significant digit (rightmost).\n   \n2. Sorting:\n   - Group the numbers based on the current digit being considered.\n   - Sort the numbers within each group.\n\n3. Repeat:\n   - Move to the next significant digit to the left and repeat the process.\n   - Continue until the most significant digit is processed.\n\nMSD Radix Sort (Most Significant Digit)\n\n1. Initialize:\n   - Start with the most significant digit (leftmost).\n\n2. Sorting:\n   - Group the numbers based on the current digit being considered.\n   - Sort the numbers within each group.\n\n3. Recursion:\n   - Recursively sort the groups using the next significant digit to the right.\n   - Continue the process until the least significant digit is processed.\n\nExample of LSD Radix Sort\n\nConsider sorting the following list of integers:\n\n\n[170, 45, 75, 90, 802, 24, 2, 66]\n\n\nStep-by-Step Process\n\n1. Sort by the least significant digit (units place):\n\n   \n   [170, 90, 802, 2, 24, 45, 75, 66]\n   \n\n2. Sort by the next significant digit (tens place):\n\n   \n   [802, 2, 24, 45, 66, 170, 75, 90]\n   \n\n3. Sort by the most significant digit (hundreds place):\n\n   \n   [2, 24, 45, 66, 75, 90, 170, 802]\n   \n\nThe list is now sorted.\n\nAdvantages of Radix Sort\n\n- Efficiency: Radix sort can be faster than comparison-based sorting algorithms (like quicksort, mergesort) when the range of the keys (digits) is not too large.\n- Simplicity: The algorithm is simple and can be implemented easily for integer keys.\n- Stability: Radix sort is stable, meaning that it preserves the relative order of records with equal keys.\n\nDisadvantages of Radix Sort\n\n- Memory Usage: Radix sort requires additional memory for temporary storage during sorting.\n- Limited Applicability: Radix sort is not suitable for all types of data, particularly for floating-point numbers or strings with varying lengths.\n\nImplementation in Java\n\nHere's a Java implementation of LSD radix sort:\n\njava\nimport java.util.Arrays;\n\npublic class RadixSort {\n\n    public static void radixSort(int[] arr) {\n        int max = Arrays.stream(arr).max().getAsInt();\n        int exp = 1;\n        while (max / exp > 0) {\n            countingSort(arr, exp);\n            exp *= 10;\n        }\n    }\n\n    private static void countingSort(int[] arr, int exp) {\n        int n = arr.length;\n        int[] output = new int[n];\n        int[] count = new int[10];\n        \n        // Store the count of occurrences in count[]\n        for (int i = 0; i < n; i++) {\n            int index = (arr[i] / exp) % 10;\n            count[index]++;\n        }\n        \n        // Change count[i] so that it contains the actual position of this digit in output[], We transform this array so that each entry at index i contains the cumulative sum of counts up to i. This tells us the position where each digit should be placed in the output array. \n        for (int i = 1; i < 10; i++) {\n            count[i] += count[i - 1];\n        }\n        \n        // Build the output array\n        for (int i = n - 1; i >= 0; i--) {\n            int index = (arr[i] / exp) % 10;\n            output[count[index] - 1] = arr[i];\n            count[index]--;\n        }\n        \n        // Copy the output array to arr[], so that arr now contains sorted numbers according to the current digit\n        System.arraycopy(output, 0, arr, 0, n);\n    }\n\n    public static void main(String[] args) {\n        int[] arr = {170, 45, 75, 90, 802, 24, 2, 66};\n        radixSort(arr);\n        System.out.println(Arrays.toString(arr)); // Output: [2, 24, 45, 66, 75, 90, 170, 802]\n    }\n}\n\n\nExplanation\n\n1. Counting Sort Function:\n   - A helper function that sorts elements based on the current digit (specified by `exp`).\n\n2. Radix Sort Function:\n   - It repeatedly calls the counting sort for each digit, starting from the least significant digit to the most significant digit.\n   - `exp` is used to isolate each digit.\n\nRadix sort can be very efficient for specific types of data, especially when the number of digits (d) and the range of digits (k) are relatively small. It avoids the (O(n log n)) lower bound of comparison-based sorting algorithms, making it useful in certain scenarios.\n\nTime and Space Complexities\n\nTime Complexity:\n- Average Case: (O(d. (n + k)))\n- Worst Case: (O(d. (n + k)))\n  - Where (d) is the number of digits in the largest number (or length of the longest string), (n) is the number of elements, and (k) is the range of the digits (e.g., 0-9 for decimal numbers).\n\nSpace Complexity:\n- Auxiliary Space: (O(n + k))\n  - Additional space is used to create buckets for grouping digits or characters. The space complexity depends on the number of elements (n) and the range of digits (k).\n",
    "Time Complexity of Radix Sort\n\nThe time complexity of Radix Sort is determined by the following factors:\n- (d): The number of digits in the largest number (or the length of the longest string if sorting strings).\n- (n): The number of elements in the input array.\n- (k): The range of the digits (for decimal numbers, (k) is 10, since digits range from 0 to 9).\n\nThe overall time complexity is given by (O(d (n + k))).\n\nBreaking it Down:\n1. Counting Sort Complexity: \n   - Counting Sort is used as a subroutine within Radix Sort.\n   - The time complexity of Counting Sort is (O(n + k)).\n\n2. Number of Passes (d):\n   - Radix Sort performs Counting Sort for each digit. If the maximum number in the array has (d) digits, then there will be (d) passes.\n   - Therefore, the total time complexity of Radix Sort is (O(d (n + k))).\n\nAnalysis:\n1. Average Case: (O(d (n + k)))\n   - This is the expected performance in general situations.\n\n2. Worst Case: (O(d (n + k)))\n   - This is also the performance in the worst-case scenario since Radix Sort does not have a different behavior for worst-case inputs as some other sorting algorithms might.\n\nComparing with Other Complexities:\n- Linear Time Complexity: (O(n))\n  - Radix Sort can achieve linear time complexity if (d) and (k) are constant values or do not grow with (n). For example, if all numbers have a fixed number of digits and the digit range is fixed.\n  \n- Quadratic Time Complexity: (O(n^2))\n  - Radix Sort does not reach (O(n^2)) time complexity unless (d) and (k) are very large in relation to (n), which is uncommon in practical scenarios. In typical uses, (d) and (k) are much smaller compared to (n), leading to a more efficient sort than (O(n^2)).\n\nBest Time Complexity:\nThe best time complexity of Radix Sort is (O(n)). This occurs under the following conditions:\n- (d) (number of digits) is a small constant.\n- (k) (range of digits) is also a small constant.\n\nIn such cases, the overall time complexity simplifies to (O(n)), since (d) and (k) do not grow with (n).\n\nSummary:\n- Average Case Time Complexity: (O(d (n + k)))\n- Worst Case Time Complexity: (O(d (n + k)))\n- Best Case Time Complexity: (O(n)) (when (d) and (k) are constants)\n\nRadix Sort is very efficient when the number of digits (d) and the range (k) are relatively small compared to the number of elements (n). It provides a linear time sorting alternative in such scenarios, outperforming typical comparison-based sorts like Quick Sort and Merge Sort which have (O(n log n)) time complexity."
  ],
  "eulerian_circuit": [
    "An Eulerian Circuit in a graph is a circuit that visits every edge exactly once and returns to the starting vertex. The necessary conditions for an Eulerian Circuit are:\n\n1. For undirected graphs: Every vertex must have an even degree, and all vertices with non-zero degree must be connected.\n2. For directed graphs: Every vertex must have equal in-degree and out-degree, and all vertices with non-zero degree must be strongly connected.\n\nHierholzerâ€™s algorithm is used to find an Eulerian path or circuit in a graph. In this problem, we adapt the algorithm to find an Eulerian path in a directed graph, where we must visit every edge (flight) exactly once. The algorithm involves traversing the graph and backtracking as necessary to cover all edges.\n\nA De Bruijn graph is a directed graph that represents overlapping subsequences of symbols from a larger sequence. It has applications in various fields, including computer science, bioinformatics, and cryptography.\n\nConsider a De Bruijn graph ðµ ( 2 , 3 ) B(2,3) over the binary alphabet {(0, 1)} {(0, 1)}. It consists of vertices representing all possible 3-bit binary sequences: 000, 001, 010, 011, 100, 101, 110, and 111. The edges are determined by the overlapping subsequences of length ð‘› âˆ’ 1 = 2.\n\nFor example, there is an edge from 001 to 010 because the last 2 bits of 001 match the first 2 bits of 010. Similarly, there is an edge from 110 to 101 because the last 2 bits of 110 match the first 2 bits of 101.\n\nJava Implementation Using Hierholzerâ€™s Algorithm\n\nExample\nHere is an example of how Hierholzerâ€™s algorithm works in a graph with an Eulerian circuit.\n\nGiven graph:\n\n    A - B\n    |     |\n    D - C\nAdjacency List:\n\nA: B, D\nB: A, C\nC: B, D\nD: A, C\n\nSteps:\n\nStart at vertex A.\nFollow an edge from A to B, removing the edge A-B.\nFollow an edge from B to C, removing the edge B-C.\nFollow an edge from C to D, removing the edge C-D.\nFollow an edge from D to A, removing the edge D-A.\nSince A has no more edges, backtrack, appending A to the path.\nContinue backtracking and appending each vertex to the path until the stack is empty.\nFinal Eulerian circuit path: A -> B -> C -> D -> A\n\nHereâ€™s how you can implement the solution in Java:\n\nGraph Representation: Use an adjacency list.\nDegree Calculation: Track in-degrees and out-degrees.\nCondition Checking: Ensure each vertex has equal in-degree and out-degree.\n\nHierholzerâ€™s Algorithm: To find the Eulerian Circuit.\n\nimport java.util.*;\n\npublic class EulerianCircuit {\n\n    // Function to perform Hierholzer's Algorithm\n    public static List<Integer> hierholzer(Map<Integer, LinkedList<Integer>> graph, int startNode) {\n        Stack<Integer> stack = new Stack<>();\n        List<Integer> circuit = new ArrayList<>();\n        stack.push(startNode);\n\n        while (!stack.isEmpty()) {\n            int u = stack.peek();\n            if (graph.get(u).size() > 0) {\n                int v = graph.get(u).removeFirst();\n                stack.push(v);\n            } else {\n                circuit.add(stack.pop());\n            }\n        }\n        Collections.reverse(circuit);\n        return circuit;\n    }\n\n    public static List<Integer> findEulerianCircuit(List<int[]> edges) {\n        Map<Integer, LinkedList<Integer>> adjList = new HashMap<>();\n        Map<Integer, Integer> inDegree = new HashMap<>();\n        Map<Integer, Integer> outDegree = new HashMap<>();\n\n        // Step 1: Build the graph from the edge list\n        for (int[] edge : edges) {\n            int u = edge[0], v = edge[1];\n            adjList.putIfAbsent(u, new LinkedList<>());\n            adjList.putIfAbsent(v, new LinkedList<>());\n            adjList.get(u).add(v);\n            outDegree.put(u, outDegree.getOrDefault(u, 0) + 1);\n            inDegree.put(v, inDegree.getOrDefault(v, 0) + 1);\n        }\n\n        // Step 2: Check the necessary conditions for an Eulerian Circuit\n        Integer startNode = null;\n        for (int node : adjList.keySet()) {\n            if (!outDegree.getOrDefault(node, 0).equals(inDegree.getOrDefault(node, 0))) {\n                return Collections.emptyList(); // No Eulerian Circuit exists\n            }\n            if (startNode == null) {\n                startNode = node;\n            }\n        }\n\n        // Step 3: Find Eulerian Circuit using Hierholzer's Algorithm\n        if (startNode == null) return Collections.emptyList();\n        return hierholzer(adjList, startNode);\n    }\n\n    public static void main(String[] args) {\n        List<int[]> edges = Arrays.asList(\n            new int[]{0, 1},\n            new int[]{1, 2},\n            new int[]{2, 0}\n        );\n\n        List<Integer> eulerianCircuit = findEulerianCircuit(edges);\n        System.out.println(eulerianCircuit);\n    }\n}\n\n\nExplanation\n\nGraph Representation:\n\nThe graph is represented using a Map<Integer, LinkedList<Integer>> where each key is a node, and the value is a list of nodes it is connected to.\ninDegree and outDegree maps keep track of the in-degree and out-degree of each node.\n\nBuilding the Graph:\n\nFor each edge (u, v), update the adjacency list, out-degree of u, and in-degree of v.\n\nChecking Conditions:\n\nEnsure each vertex has equal in-degree and out-degree. If any vertex fails this condition, an Eulerian Circuit does not exist.\nIdentify a start node for the algorithm.\n\nHierholzerâ€™s Algorithm:\n\nUse a stack to traverse the graph starting from the identified start node. Remove edges as they are traversed to avoid revisiting them.\nWhen you cannot continue from a node, backtrack and add the node to the circuit list.\nFinally, reverse the circuit list to get the correct order of traversal.\n\nMain Method:\n\nDemonstrates calling findEulerianCircuit with a sample edge list and prints the resulting Eulerian Circuit.\n"
  ],
  "strongly_connected component": [
    "In graph theory, a strongly connected component (SCC) of a directed graph is a maximal strongly connected subgraph. That is, a subgraph where there is a path from each vertex to every other vertex within the subgraph. SCCs are significant in various applications like understanding the structure of networks, optimizing compilers, and analyzing social networks.\n\nKosaraju's Algorithm in Java\n\nKosaraju's Algorithm consists of two main phases. Here is a detailed implementation in Java:\n\nimport java.util.*;\n    \npublic class KosarajuSCC {\n        \n        // First DFS to fill stack according to finishing times\n        private void fillOrder(int v, boolean[] visited, Stack<Integer> stack, List<List<Integer>> adj) {\n            visited[v] = true;\n            for (int neighbor : adj.get(v)) {\n                if (!visited[neighbor]) {\n                    fillOrder(neighbor, visited, stack, adj);\n                }\n            }\n            stack.push(v);\n        }\n    \n        // Transpose the graph\n        private List<List<Integer>> getTranspose(List<List<Integer>> adj, int V) {\n            List<List<Integer>> transpose = new ArrayList<>();\n            for (int i = 0; i < V; i++) {\n                transpose.add(new ArrayList<>());\n            }\n            for (int v = 0; v < V; v++) {\n                for (int neighbor : adj.get(v)) {\n                    transpose.get(neighbor).add(v);\n                }\n            }\n            return transpose;\n        }\n    \n        // DFS for the transposed graph\n        private void DFSUtil(int v, boolean[] visited, List<List<Integer>> adj, List<Integer> component) {\n            visited[v] = true;\n            component.add(v);\n            for (int neighbor : adj.get(v)) {\n                if (!visited[neighbor]) {\n                    DFSUtil(neighbor, visited, adj, component);\n                }\n            }\n        }\n    \n        // Function to find and print all SCCs\n        public List<List<Integer>> stronglyConnectedComponents(int V, List<List<Integer>> adj) {\n            Stack<Integer> stack = new Stack<>();\n            boolean[] visited = new boolean[V];\n    \n            // Fill vertices in stack according to their finishing times\n            for (int i = 0; i < V; i++) {\n                if (!visited[i]) {\n                    fillOrder(i, visited, stack, adj);\n                }\n            }\n    \n            // Create a transposed graph\n            List<List<Integer>> transpose = getTranspose(adj, V);\n    \n            // Mark all vertices as not visited (for second DFS)\n            Arrays.fill(visited, false);\n            List<List<Integer>> sccs = new ArrayList<>();\n    \n            // Process all vertices in order defined by Stack\n            while (!stack.isEmpty()) {\n                int v = stack.pop();\n                if (!visited[v]) {\n                    List<Integer> component = new ArrayList<>();\n                    DFSUtil(v, visited, transpose, component);\n                    sccs.add(component);\n                }\n            }\n            return sccs;\n        }\n    \n        public static void main(String[] args) {\n            KosarajuSCC kosaraju = new KosarajuSCC();\n            int V = 5;\n            List<List<Integer>> adj = new ArrayList<>();\n            for (int i = 0; i < V; i++) {\n                adj.add(new ArrayList<>());\n            }\n    \n            adj.get(0).add(2);\n            adj.get(2).add(1);\n            adj.get(1).add(0);\n            adj.get(0).add(3);\n            adj.get(3).add(4);\n    \n            List<List<Integer>> sccs = kosaraju.stronglyConnectedComponents(V, adj);\n            System.out.println('Strongly Connected Components:');\n            for (List<Integer> scc : sccs) {\n                System.out.println(scc);\n            }\n        }\n    }\n\nTarjan's Algorithm in Java\n\nTarjan's Algorithm is a single-pass DFS algorithm that uses low-link values to identify SCCs.\n\nimport java.util.*;\n    \npublic class TarjanSCC {\n        private int time = 0;\n        private List<List<Integer>> sccs = new ArrayList<>();\n    \n        // Function to perform DFS and find SCCs using Tarjan's Algorithm\n        private void tarjanDFS(int v, int[] disc, int[] low, Deque<Integer> stack, boolean[] inStack, List<List<Integer>> adj) {\n            disc[v] = low[v] = ++time;\n            stack.push(v);\n            inStack[v] = true;\n    \n            for (int neighbor : adj.get(v)) {\n                if (disc[neighbor] == -1) {\n                    tarjanDFS(neighbor, disc, low, stack, inStack, adj);\n                    low[v] = Math.min(low[v], low[neighbor]);\n                } else if (inStack[neighbor]) {\n                    low[v] = Math.min(low[v], disc[neighbor]);\n                }\n            }\n    \n            if (low[v] == disc[v]) {\n                List<Integer> component = new ArrayList<>();\n                int w;\n                do {\n                    w = stack.pop();\n                    inStack[w] = false;\n                    component.add(w);\n                } while (w != v);\n                sccs.add(component);\n            }\n        }\n    \n        public List<List<Integer>> stronglyConnectedComponents(int V, List<List<Integer>> adj) {\n            int[] disc = new int[V];\n            int[] low = new int[V];\n            boolean[] inStack = new boolean[V];\n            Deque<Integer> stack = new ArrayDeque<>();\n    \n            Arrays.fill(disc, -1);\n            Arrays.fill(low, -1);\n    \n            for (int i = 0; i < V; i++) {\n                if (disc[i] == -1) {\n                    tarjanDFS(i, disc, low, stack, inStack, adj);\n                }\n            }\n            return sccs;\n        }\n    \n        public static void main(String[] args) {\n            TarjanSCC tarjan = new TarjanSCC();\n            int V = 5;\n            List<List<Integer>> adj = new ArrayList<>();\n            for (int i = 0; i < V; i++) {\n                adj.add(new ArrayList<>());\n            }\n    \n            adj.get(0).add(2);\n            adj.get(2).add(1);\n            adj.get(1).add(0);\n            adj.get(0).add(3);\n            adj.get(3).add(4);\n    \n            List<List<Integer>> sccs = tarjan.stronglyConnectedComponents(V, adj);\n            System.out.println('Strongly Connected Components:');\n            for (List<Integer> scc : sccs) {\n                System.out.println(scc);\n            }\n        }\n    }\n\nExplanation\n\nKosaraju's Algorithm:\n\nFirst DFS:\nTraverse the graph and push vertices onto a stack when finished (finishing time).\n\nTranspose Graph:\nReverse the direction of all edges.\n\nSecond DFS:\nPop vertices from the stack and perform DFS on the transposed graph. Each DFS tree gives one SCC.\n\nTarjan's Algorithm:\n\nDFS with Low-Link Values:\nPerform a single DFS traversal. Maintain discovery times and low-link values.\n\nUse a stack to keep track of the current path in the DFS tree.\n\nIf a node's low-link value equals its discovery time, it is the root of an SCC.\n\nComparing the Algorithms:\n\nKosaraju's Algorithm:\nSimpler to understand and implement.\nRequires two passes of the graph (two DFS traversals).\nUses an auxiliary stack to record finishing order.\n\nTarjan's Algorithm:\nMore efficient with a single pass of the graph.\nUses low-link values to identify SCCs.\nSlightly more complex due to the management of low-link values and the stack.\n\nBoth algorithms have a time complexity of O(V+E) and are efficient for large graphs. The choice depends on the specific problem constraints and personal preference for implementation complexity."
  ],
  "reservoir_sampling": [
    "Reservoir Sampling is a clever algorithm used for selecting a random sample of k items from an unknown or large stream of items, where the total number of items is not known in advance. It ensures that each item in the stream has an equal probability of being selected in the final sample.\n\nHere's a Python implementation of Reservoir Sampling for selecting k elements from a list:\n    \n    import random\n    \n    def reservoir_sampling(stream, k):\n        reservoir = []\n        \n        for i in range(k):\n            reservoir.append(stream[i])\n        \n       for i in range(k, len(stream)):\n           j = random.randint(0, i)\n            \n           if j < k:\n                reservoir[j] = stream[i]\n        \n        return reservoir\n    \n    stream = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n    k = 3\n    sample = reservoir_sampling(stream, k)\n    print('Random sample of size', k, ':', sample)\n\nIn this implementation, we maintain a reservoir of size k. We fill the reservoir with the first k elements from the stream. Then, for each subsequent element in the stream, we generate a random index between 0 and the current index. If the random index is less than k, we replace the element at that index in the reservoir with the current element from the stream. This way, each element in the stream has an equal probability of being included in the final sample.",
    "Rejection sampling is focused on generating samples from a specific distribution using a proposal distribution, while reservoir sampling is designed to handle the random selection of a fixed number of items from a potentially large and unknown-sized data stream."
  ],
  "line_sweep": [
    "A line sweep algorithm is a powerful computational geometry technique used to solve various problems efficiently by sweeping a line (or a plane in higher dimensions) across the plane to process geometric entities in a certain order. This technique is particularly useful for problems involving intersections, such as detecting if any two intervals overlap, finding the closest pair of points, or computing the union of rectangles.\n\nKey Concepts of Line Sweep:\n\nSweep Line: Imagine a line (usually vertical) sweeping across the plane from left to right (or bottom to top). As it moves, it processes events at specific positions.\n\nEvents: These are the points where the status of the sweep line changes. For example, in the case of interval overlap detection, events can be the starting and ending points of the intervals.\n\nEvent Queue: A priority queue that stores all events sorted by their position along the sweep direction. This ensures that events are processed in the correct order.\n\nStatus Structure: A data structure that maintains the current status of the sweep line, often implemented as a balanced binary search tree (BST). For example, in the case of interval overlap detection, the status structure could store the active intervals.\n\nExample Problems and Solutions:\n\n1. Interval Overlap Detection\n\nGiven a set of intervals, determine if any two intervals overlap.\n\nSteps:\n\nCreate events for all interval start and end points.\nSort the events based on their position.\nUse a status structure to keep track of active intervals.\nAs you process each event:\nIf it's a start event, check for overlaps with the active intervals.\nIf it's an end event, remove the corresponding interval from the status structure.\n\n2. Finding Closest Pair of Points\nGiven a set of points in the plane, find the pair with the smallest distance.\n\nSteps:\n\nSort points by their x-coordinates.\nSweep a vertical line across the points.\nUse a status structure (like a BST) to keep track of points in a vertical strip.\nFor each point, only consider points within a distance equal to the current smallest distance.\n\nExample: Interval Overlap Detection\n\nLet's illustrate this with a simple Python example for interval overlap detection.\n\nclass Event:\n    def __init__(self, x, is_start, interval):\n        self.x = x\n        self.is_start = is_start\n        self.interval = interval\n\ndef interval_overlap(intervals):\n    events = []\n    for interval in intervals:\n        events.append(Event(interval[0], True, interval))\n        events.append(Event(interval[1], False, interval))\n    \n    events.sort(key=lambda e: (e.x, not e.is_start))\n    \n    active_intervals = set()\n    for event in events:\n        if event.is_start:\n            for active in active_intervals:\n                if event.interval[0] < active[1]:\n                    return True\n            active_intervals.add(event.interval)\n        else:\n            active_intervals.remove(event.interval)\n    \n    return False\n\n Example usage:\nintervals = [(1, 3), (2, 4), (5, 7)]\nprint(interval_overlap(intervals))   Output: True\n\nIn this example, we create events for each interval's start and end points, sort them, and then use a set to maintain the currently active intervals. If we find an overlap, we return True; otherwise, we return False.\n\nLine sweep algorithms are versatile and can be adapted to many other computational geometry problems."
  ],
  "shell_sort": [
    "Shell sort is a classic sorting algorithm that is an improvement over insertion sort. It sorts elements at a gap, reducing the gap over time until it performs a final insertion sort at gap 1.",
    "Shell sort is an in-place comparison sort that is also known as Shell sort or Shell's method.",
    "It can be shown as a generalization of either exchange bubble sorting or insertion sorting.",
    "The method begins by sorting pairs of elements far apart from each other, then gradually narrows the gap between elements to be compared. ",
    "Some out-of-place elements can be moved into position faster than what a simple nearest-neighbor exchange would, by starting with far apart elements.",
    "This algorithm uses insertion sort on widely spread elements to sort them and then sort the less widely spaced elements. This spacing is termed as the interval. ",
    "Hereâ€™s how you can implement Shell sort:\n\ndef shellSort(arr):\n    n = len(arr)\n    gap = n // 2\n\n     Start with a big gap, then reduce the gap\n    while gap > 0:\n        for i in range(gap, n):\n            temp = arr[i]\n\n             Shift earlier gap-sorted elements up until the correct location for arr[i] is found\n            j = i\n            while j >= gap and arr[j - gap] > temp:\n                arr[j] = arr[j - gap]\n                j -= gap\n\n             Put temp (the original arr[i]) in its correct location\n            arr[j] = temp\n\n        gap //= 2\n\narr = [12, 34, 54, 2, 3]\nshellSort(arr)\nprint('Sorted array is:', arr)\n\nThis implementation uses the Shell sort algorithm, which starts by comparing elements a certain gap apart and reduces the gap until it performs a final pass with gap 1. Here's a step-by-step explanation:\n\n1. Initialize the Gap: Start with a large gap and reduce it by half in each iteration.\n2. Sort Subarrays: For each gap size, sort the subarrays formed by taking every gap-th element.\n3. Insertion Sort: Use a modified insertion sort that operates over the gap-separated elements.\n4. Reduce the Gap: Continue reducing the gap and repeat the sorting process until the gap is 0.\n\nThis algorithm works well for medium-sized arrays and offers better performance than simple insertion sort due to its initial larger gap sizes, which help in moving elements closer to their final positions faster.",
    "Time Complexity\n\nThe time complexity of Shell sort is difficult to precisely determine due to its dependence on the chosen gap sequence. However, here are some general observations:\n\n1. Worst-case Time Complexity: \n   - For the original Shell's gap sequence (N/2, N/4, ..., 1), the worst-case time complexity is O(N^2).\n   - Using more efficient gap sequences, such as Hibbard's gaps (1, 3, 7, ..., 2^k - 1), the worst-case time complexity can be improved to O(N^{3/2}).\n   - Sedgewick's gaps can yield a worst-case time complexity of O(N^{4/3}).\n   - Using the best known gap sequence, the worst-case time complexity can be as good as O(N log^2 N).\n\n2. Average-case Time Complexity: \n   - With the original Shell's gap sequence, the average-case time complexity is around O(N^{5/4}).\n   - With more advanced gap sequences, the average-case time complexity is often closer to O(N log N).\n\n3. Best-case Time Complexity:\n   - The best-case time complexity is O(N log N), occurring when the input array is already nearly sorted, particularly with efficient gap sequences.\n\nSpace Complexity\n\nThe space complexity of Shell sort is straightforward:\n\n- Space Complexity: O(1)\n\nShell sort is an in-place sorting algorithm, meaning it requires a constant amount of additional space regardless of the input size.\n\nSummary\n\n- Worst-case Time Complexity: Ranges from O(N^2) (with simple gap sequences) to O(N log^2 N) (with advanced gap sequences).\n- Average-case Time Complexity: Generally closer to O(N log N) with efficient gap sequences.\n- Best-case Time Complexity: O(N log N).\n- Space Complexity: O(1) (in-place sorting).\n\nThe choice of gap sequence plays a crucial role in determining the efficiency of Shell sort, with more sophisticated sequences significantly improving its performance."
  ],
  "shell": [
    "Unix commands are essential tools for navigating, manipulating, and managing files and processes in a Unix-like operating system.\n\nThese problems require knowledge of Unix/Linux shell commands and scripting. :\n\nFile and Directory Management\n\n1. `ls`: Lists directory contents.\n   - `ls`: List files and directories in the current directory.\n   - `ls -l`: List in long format, showing file details.\n   - `ls -a`: List all files, including hidden files.\n\n2. `cd`: Changes the current directory.\n   - `cd /path/to/directory`: Change to specified directory.\n   - `cd ..`: Move up one directory level.\n   - `cd ~`: Change to the home directory.\n\n3. `pwd`: Prints the working directory.\n   - `pwd`: Displays the current directory path.\n\n4. `mkdir`: Creates a new directory.\n   - `mkdir new_directory`: Create a directory named `new_directory`.\n\n5. `rmdir`: Removes an empty directory.\n   - `rmdir directory`: Remove the specified empty directory.\n\n6. `rm`: Removes files or directories.\n   - `rm file`: Remove the specified file.\n   - `rm -r directory`: Remove the specified directory and its contents recursively.\n   - `rm -f file`: Force remove a file without prompting.\n\n7. `cp`: Copies files or directories.\n   - `cp source destination`: Copy file from source to destination.\n   - `cp -r source_directory destination`: Copy directory and its contents.\n\n8. `mv`: Moves or renames files or directories.\n   - `mv old_name new_name`: Rename a file or directory.\n   - `mv source destination`: Move file or directory to a new location.\n\n9. `ln`: Creates links between files.\n   - `ln source target`: Create a hard link.\n   - `ln -s source target`: Create a symbolic (soft) link.\n\nFile Content and Manipulation\n\n1. `cat`: Concatenates and displays file content.\n   - `cat file`: Display the content of a file.\n   - `cat file1 file2`: Concatenate and display multiple files.\n\n2. `more` and `less`: Views file content one screen at a time.\n   - `more file`: View file content with forward navigation.\n   - `less file`: View file content with forward and backward navigation.\n\n3. `head`: Outputs the first part of files.\n   - `head file`: Display the first 10 lines of a file by default.\n   - `head -n 5 file`: Display the first 5 lines of a file.\n\n4. `tail`: Outputs the last part of files.\n   - `tail file`: Display the last 10 lines of a file by default.\n   - `tail -n 5 file`: Display the last 5 lines of a file.\n\n5. `grep`: Searches for patterns within files.\n   - `grep 'pattern' file`: Search for a pattern in a file.\n   - `grep -r 'pattern' directory`: Recursively search for a pattern in a directory.\n\n6. `awk`: Pattern scanning and processing language.\n   - `awk '{print $1}' file`: Print the first column of each line.\n   - `awk '/pattern/ {print $0}' file`: Print lines matching the pattern.\n\n7. `sed`: Stream editor for filtering and transforming text.\n   - `sed 's/old/new/g' file`: Replace `old` with `new` globally in a file.\n   - `sed -n '10p' file`: Print the 10th line of a file.\n\n8. `tr`: Translates or deletes characters.\n   - `tr -s ' ' '\\n' < words.txt`: Translates spaces to newlines, effectively turning the file into a list of words.\n   - Common usage in conjunction with other commands for text processing.\n\nWord Frequency Count Example\n\nThis is a common task where you might want to count the frequency of each word in a text file:\n\nsh\ntr -s ' ' '\\n' < words.txt | sort | uniq -c | sort -nr\n\n\n- `tr -s ' ' '\\n' < words.txt`: Translates spaces into newlines, effectively turning the file into a list of words.\n- `sort`: Sorts the list of words.\n- `uniq -c`: Counts the occurrences of each unique word.\n- `sort -nr`: Sorts the counts in descending numerical order.\n\nFile Permissions and Ownership\n\n1. `chmod`: Changes file permissions.\n   - `chmod 755 file`: Set permissions to `rwxr-xr-x`.\n   - `chmod +x file`: Add execute permission.\n\n2. `chown`: Changes file owner and group.\n   - `chown user:group file`: Change the owner and group of a file.\n   - `chown user file`: Change only the owner of a file.\n\nProcess Management\n\n1. `ps`: Displays information about active processes.\n   - `ps`: Display current user's active processes.\n   - `ps aux`: Display detailed information about all processes.\n\n2. `top`: Displays real-time system summary and process list.\n   - `top`: Show real-time process information.\n\n3. `kill`: Terminates processes.\n   - `kill PID`: Send the default `TERM` signal to the process with `PID`.\n   - `kill -9 PID`: Forcefully terminate the process with `PID`.\n\n4. `bg` and `fg`: Manages background and foreground processes.\n   - `bg`: Resume a suspended job in the background.\n   - `fg`: Bring a background job to the foreground.\n\nNetworking\n\n1. `ping`: Checks network connectivity to a host.\n   - `ping hostname`: Send ICMP echo requests to the host.\n\n2. `ifconfig`: Configures network interfaces (deprecated, use `ip`).\n   - `ifconfig`: Display network interfaces and their statuses.\n\n3. `curl`: Transfers data from or to a server.\n   - `curl http://example.com`: Fetch content from a URL.\n\n4. `wget`: Retrieves files from the web.\n   - `wget http://example.com/file`: Download a file from a URL.\n\nDisk Usage\n\n1. `df`: Reports file system disk space usage.\n   - `df -h`: Display disk space usage in human-readable format.\n\n2. `du`: Estimates file space usage.\n   - `du -sh directory`: Display total size of a directory in human-readable format.\n\nArchiving and Compression\n\n1. `tar`: Archives files.\n   - `tar -cvf archive.tar directory`: Create a tar archive.\n   - `tar -xvf archive.tar`: Extract a tar archive.\n\n2. `gzip`: Compresses files.\n   - `gzip file`: Compress a file to `file.gz`.\n   - `gunzip file.gz`: Decompress a `.gz` file.\n\nConclusion\n\nThese Unix commands cover a broad range of tasks from file and directory management to process control, text processing, networking, and system monitoring. Mastering these commands can greatly enhance your efficiency and capability in managing Unix/Linux systems."
  ],
  "counting_sort": [
    "Counting Sort is a linear time sorting algorithm that sorts elements by counting the number of occurrences of each unique element in the input array. It's particularly useful when the range of input elements is known ahead of time and is not significantly larger than the number of elements to be sorted.\n\nHere's an implementation of Counting Sort:\n\ndef counting_sort(arr):\n    if len(arr) == 0:\n        return []\n\n    max_element = max(arr)\n\n     Create a counting array to store the count of each element\n    count = [0] * (max_element + 1)\n\n     Count the occurrences of each element in the input array\n    for num in arr:\n        count[num] += 1\n\n     Reconstruct the sorted array using the counting array\n    sorted_arr = []\n    for i in range(len(count)):\n        sorted_arr.extend([i] * count[i])\n\n    return sorted_arr\n\nprint(counting_sort([2, 2, 3, 1]))\n\n[0, 1, 2, 1]\n\n[]\n[1]\n[2, 2]\n[3]\n[1, 2, 2, 3]\n \n\nThis implementation assumes that the input array `arr` consists of non-negative integers. It first finds the maximum element in the array to determine the size of the counting array. Then, it counts the occurrences of each element in the input array and reconstructs the sorted array based on the counts.\n\nCounting Sort has a time complexity of O(n + k), where n is the number of elements in the input array and k is the range of input values. It's not a comparison-based sorting algorithm, making it faster than comparison-based algorithms like Quick Sort or Merge Sort for certain types of input data, especially when the range of input values is small.\n\nFor LeetCode problems related to sorting algorithms, you can often adapt or directly use these implementations depending on the problem requirements and constraints.",
    "Time Complexity:\n\nCounting the occurrences of each element in the input array takes O(n) time, where n is the number of elements in the array.\nReconstructing the sorted array from the counting array takes O(n + k) time, where k is the range of input values.\nOverall, the time complexity is O(n + k).\nSpace Complexity:\n\nThe space complexity is dominated by the counting array, which requires O(k) additional space, where k is the range of input values.\nIn addition to the input array and the counting array, the algorithm uses a constant amount of extra space for variables and temporary arrays.\nThus, the space complexity is O(n + k).\nIn summary:\n\nTime Complexity: O(n + k)\nSpace Complexity: O(n + k)\nCounting Sort's space complexity can be problematic for large ranges of input values since it requires memory proportional to the range of input values. However, if the range is relatively small compared to the number of elements in the array (i.e., k is much smaller than n), Counting Sort can be an efficient sorting algorithm.\n"
  ],
  "minimum_spanning tree": [
    "A Minimum Spanning Tree (MST) of a weighted, connected, undirected graph is a spanning tree whose sum of edge weights is as small as possible. The spanning tree includes all the vertices of the graph, and the total weight of all the edges in the tree is minimized.",
    "Work In Progress..."
  ],
  "suffix_array": [
    "A suffix array is a powerful data structure used in string processing. It is an array of integers that represents the starting positions of the suffixes of a string, sorted in lexicographical order. \n\nKey Points\n\n1. Suffix: A suffix of a string is any substring that starts from a certain position and goes to the end of the string. For example, the suffixes of the string \"banana\" are \"banana\", \"anana\", \"nana\", \"ana\", \"na\", and \"a\".\n\n2. Suffix Array: The suffix array of a string is an array of the starting indices of the suffixes, sorted in lexicographical order of the suffixes themselves. \n\nExample\n\nConsider the string \"banana\":\n\n- Suffixes:\n  - \"banana\" (starting at index 0)\n  - \"anana\" (starting at index 1)\n  - \"nana\" (starting at index 2)\n  - \"ana\" (starting at index 3)\n  - \"na\" (starting at index 4)\n  - \"a\" (starting at index 5)\n\n- Sorted Suffixes:\n  - \"a\" (starting at index 5)\n  - \"ana\" (starting at index 3)\n  - \"anana\" (starting at index 1)\n  - \"banana\" (starting at index 0)\n  - \"na\" (starting at index 4)\n  - \"nana\" (starting at index 2)\n\n- Suffix Array:\n  - [5, 3, 1, 0, 4, 2]\n\nThe suffix array for \"banana\" is `[5, 3, 1, 0, 4, 2]`.\n\nProperties\n\n1. Efficiency: The suffix array can be constructed in (O(nlogn)) time using efficient algorithms.\n2. Space Complexity: It takes linear space relative to the size of the string.\n3. Applications: It is used in various applications such as substring search, pattern matching, data compression (like Burrows-Wheeler transform), and bioinformatics for DNA sequence analysis.\n\nConstruction of Suffix Array\n\nSteps\n\n1. Generate All Suffixes: Create all possible suffixes of the string.\n2. Sort Suffixes: Sort these suffixes lexicographically.\n3. Store Indices: Store the starting indices of these sorted suffixes in an array.\n\nJava Implementation\n\nHere's how you can implement a suffix array in Java:\n\nimport java.util.Arrays;\n\npublic class SuffixArray {\n\n    static class Suffix implements Comparable<Suffix> {\n        int index;\n        String suffix;\n\n        Suffix(int index, String suffix) {\n            this.index = index;\n            this.suffix = suffix;\n        }\n\n        @Override\n        public int compareTo(Suffix other) {\n            return this.suffix.compareTo(other.suffix);\n        }\n    }\n\n    public static int[] buildSuffixArray(String s) {\n        int n = s.length();\n        Suffix[] suffixes = new Suffix[n];\n\n        for (int i = 0; i < n; i++) {\n            suffixes[i] = new Suffix(i, s.substring(i));\n        }\n\n        // Step 2: Sort the suffixes lexicographically\n        Arrays.sort(suffixes);\n\n        int[] suffixArray = new int[n];\n        for (int i = 0; i < n; i++) {\n            suffixArray[i] = suffixes[i].index;\n        }\n\n        return suffixArray;\n    }\n\n    public static void main(String[] args) {\n        String s = \"banana\";\n        int[] suffixArray = buildSuffixArray(s);\n        System.out.println(\"Suffix array: \" + Arrays.toString(suffixArray));\n    }\n}\n\nExplanation of the Code\n\n- Suffix Class:\n  - Holds each suffix and its starting index.\n  - Implements `Comparable` to enable sorting based on the suffix string.\n\n- buildSuffixArray Function:\n  - Step 1: Generate all suffixes by iterating through each character in the string.\n  - Step 2: Sort the suffixes lexicographically using Javaâ€™s `Arrays.sort` function.\n  - Step 3: Extract and store the starting indices of the sorted suffixes.\n\nConclusion\n\nThe suffix array is an efficient and versatile data structure for string processing tasks. It provides a compact way to index all suffixes of a string, enabling fast search and analysis operations.",
    "Time Complexity\n\n1. Naive Approach:\n    - Generating all suffixes: (O(n^2))\n    - Sorting suffixes: (O(n^2 log n))\n    - Total Time Complexity: (O(n^2 log n))\n\n2. Efficient Algorithms:\n    - Manber and Myers Algorithm: (O(n log n))\n    - KÃ¤rkkÃ¤inen-Sanders Algorithm: (O(n))\n    - Total Time Complexity: (O(n log n)) or (O(n))\n\nSpace Complexity\n\n1. Naive Approach:\n    - Storing all suffixes: (O(n^2))\n    - Suffix Array: (O(n))\n    - Total Space Complexity: (O(n^2))\n\n2. Efficient Algorithms:\n    - Storing suffix array and auxiliary arrays: (O(n))\n    - Total Space Complexity: (O(n))\n\nSummary\n\n- Naive Approach:\n    - Time Complexity: (O(n^2 log n))\n    - Space Complexity: (O(n^2))\n\n- Efficient Algorithms:\n    - Time Complexity: (O(n log n)) or (O(n))\n    - Space Complexity: (O(n))\n\nEfficient algorithms significantly improve performance, making them more practical for large inputs. The naive approach is easier to understand but less practical for large strings due to higher time and space complexities.",
    "To construct the LCP (Longest Common Prefix) array using the suffix array, we need to follow a systematic approach after building the suffix array itself. Hereâ€™s how we can do it:\n\nSteps to Build LCP Array Using Suffix Array\n\n1. Sort Suffixes:\n   - First, generate the suffixes from the string `s`, each suffix represented by its starting index.\n   - Sort these suffixes lexicographically. This sorting step establishes the order of suffixes based on their starting characters.\n\n2. Build Suffix Array:\n   - From the sorted suffixes, create a suffix array that holds the indices of the sorted suffixes.\n\n3. Compute Rank Array:\n   - Construct a rank array where `rank[i]` gives the position of the suffix starting at index `i` in the sorted order of suffixes.\n\n4. Compute LCP Array:\n   - Initialize an array `lcp` of length `n-1`, where `n` is the length of the string `s`.\n   - Iterate through the suffix array to compute the LCP values between consecutive suffixes in the sorted order.\n   - Use the rank array to efficiently compare consecutive suffixes and determine their common prefix length.\n\nExample Implementation\n\nHereâ€™s a Java implementation that builds the suffix array and computes the LCP array:\n\njava\nimport java.util.*;\n\npublic class Solution {\n\n    public static void main(String[] args) {\n        String s = \"banana\";\n        int[] sa = buildSuffixArray(s);\n        int[] lcp = buildLCPArray(s, sa);\n        System.out.println(\"Suffix Array: \" + Arrays.toString(sa));\n        System.out.println(\"LCP Array: \" + Arrays.toString(lcp));\n    }\n\n    public static int[] buildSuffixArray(String s) {\n        int n = s.length();\n        Suffix[] suffixes = new Suffix[n];\n\n        for (int i = 0; i < n; i++) {\n            suffixes[i] = new Suffix(i, s.substring(i));\n        }\n\n        Arrays.sort(suffixes);\n\n        int[] suffixArray = new int[n];\n        for (int i = 0; i < n; i++) {\n            suffixArray[i] = suffixes[i].index;\n        }\n\n        return suffixArray;\n    }\n\n    public static int[] buildLCPArray(String s, int[] suffixArray) {\n        int n = s.length();\n        int[] rank = new int[n];\n        int[] lcp = new int[n - 1];\n\n        for (int i = 0; i < n; i++) {\n            rank[suffixArray[i]] = i;\n        }\n\n        int h = 0;\n        for (int i = 0; i < n; i++) {\n            if (rank[i] > 0) {\n                int j = suffixArray[rank[i] - 1];\n                while (i + h < n && j + h < n && s.charAt(i + h) == s.charAt(j + h)) {\n                    h++;\n                }\n                lcp[rank[i] - 1] = h;\n                if (h > 0) {\n                    h--;\n                }\n            }\n        }\n\n        return lcp;\n    }\n\n    static class Suffix implements Comparable<Suffix> {\n        int index;\n        String suffix;\n\n        Suffix(int index, String suffix) {\n            this.index = index;\n            this.suffix = suffix;\n        }\n\n        @Override\n        public int compareTo(Suffix other) {\n            return this.suffix.compareTo(other.suffix);\n        }\n    }\n}\n\n\nExplanation\n\n- Suffix Array Construction:\n  - `buildSuffixArray(String s)` method constructs an array of suffixes sorted lexicographically and then builds a suffix array using the indices of these sorted suffixes.\n\n- LCP Array Construction:\n  - `buildLCPArray(String s, int[] suffixArray)` method computes the LCP array using the constructed suffix array and the rank of suffixes.\n  - It iterates through the suffix array, comparing consecutive suffixes based on their ranks to determine the length of their common prefix.\n\n- Rank Array Usage:\n  - `rank` array is crucial in `buildLCPArray` to quickly find the position of each suffix in the sorted order and thus efficiently compute LCP values.\n\nThis approach ensures efficient computation of the LCP array using the suffix array, making it suitable for applications like finding the longest common prefix among all suffixes of a string efficiently."
  ],
  "bucket_sort": [
    "Bucket sort is a sorting algorithm that works well when the input is uniformly distributed over a range. It divides the input array into a number of buckets, each of which is then sorted individually, typically with another sorting algorithm or by recursively applying bucket sort itself if necessary. After sorting each bucket, the sorted buckets are concatenated to obtain the sorted array.\n\nHere's a step-by-step breakdown of how bucket sort typically works:\n\n1. Divide into Buckets: Create an array of buckets, where each bucket is responsible for a subrange of the input elements.\n\n2. Distribution: Iterate through the input array, placing each element into its corresponding bucket based on some function that maps elements to buckets. This function should distribute elements uniformly across the buckets.\n\n3. Sort Buckets: Sort each individual bucket. This can be done using any suitable sorting algorithm, such as insertion sort, quicksort, or recursively applying bucket sort itself.\n\n4. Concatenate Buckets: Concatenate all sorted buckets to get the sorted array.\n\nimport java.util.ArrayList;\nimport java.util.Collections;\nimport java.util.List;\n\npublic class BucketSort {\n\n    public static void bucketSort(float[] arr) {\n        int n = arr.length;\n\n        List<Float>[] buckets = new ArrayList[n];\n        for (int i = 0; i < n; i++) {\n            buckets[i] = new ArrayList<>();\n        }\n\n        for (int i = 0; i < n; i++) {\n            int bucketIndex = (int) (arr[i] * n);\n            buckets[bucketIndex].add(arr[i]);\n        }\n\n        for (int i = 0; i < n; i++) {\n            Collections.sort(buckets[i]); \n        }\n\n        int index = 0;\n        for (int i = 0; i < n; i++) {\n            for (float num : buckets[i]) {\n                arr[index++] = num;\n            }\n        }\n    }\n\n    public static void main(String[] args) {\n        float[] arr = {0.42f, 0.32f, 0.88f, 0.23f, 0.65f, 0.11f, 0.55f};\n\n        System.out.println(\"Array before sorting:\");\n        printArray(arr);\n\n        bucketSort(arr);\n\n        System.out.println(\"Array after sorting:\");\n        printArray(arr);\n    }\n\n    public static void printArray(float[] arr) {\n        for (float num : arr) {\n            System.out.print(num + ",
    ");\n        }\n        System.out.println();\n    }\n}\n\nExplanation:\n\n1. Initialization: \n   - `bucketSort(float[] arr)`: This method accepts an array of floats and performs bucket sort on it.\n   - `buckets` array is initialized as an array of `ArrayList<Float>`, where each index corresponds to a bucket.\n\n2. Distribution:\n   - Each element in the input array `arr` is placed into its respective bucket. The bucket index is determined by multiplying the element by `n` (number of elements) and taking the integer part.\n\n3. Sorting:\n   - Each bucket is sorted individually. In this implementation, `Collections.sort()` is used for simplicity. Other sorting algorithms like insertion sort or quicksort could be used depending on the specific requirements or characteristics of the data.\n\n4. Concatenation:\n   - Finally, the sorted elements from each bucket are concatenated back into the original array `arr`.\n\n5. Main Method:\n   - `main(String[] args)`: Example usage demonstrating how to use `bucketSort` method on a sample array of floats.\n   - `printArray(float[] arr)`: Utility method to print the contents of an array.\n\nNotes:\n- This implementation assumes sorting floating-point numbers between 0 and 1. For sorting integers or other ranges, adjustments to the bucket index calculation and handling of elements may be necessary.\n- The choice of sorting algorithm within each bucket (`Collections.sort()` in this case) can be tailored based on performance requirements and characteristics of the data.\n- Bucket sort's performance can be influenced by the number of buckets chosen (`n` in this implementation) and the distribution of input data. Adjusting these parameters can optimize the algorithm for specific use cases.\n\nTime Complexity\n\n- Best Case: If the elements are uniformly distributed across buckets and each bucket is efficiently sorted, the time complexity can be close to ( O(n) ), where ( n ) is the number of elements.\n  \n- Average Case: In the average case, bucket sort typically runs in ( O(n + k) ), where ( n ) is the number of elements to be sorted and ( k ) is the number of buckets. Sorting each bucket takes ( O(n_i log n_i) ) on average, where ( n_i ) is the number of elements in bucket ( i ).\n  \n- Worst Case: In the worst case, when all elements are placed in a single bucket, it can degrade to ( O(n^2) ) due to the time required to sort each bucket individually.\n\nSpace Complexity\n\n- Bucket sort requires additional space to store the buckets, making its space complexity ( O(n + k) ), where ( n ) is the number of elements and ( k ) is the number of buckets. This space is needed to hold the elements in each bucket before they are concatenated back into the sorted array.\n\nCertainly! Here's a simple Java implementation of bucket sort. In this example, I'll assume we're sorting an array of floating-point numbers between 0 and 1, but bucket sort can be adapted to sort integers or other data types as well.\n\n\nSummary\n\nBucket sort is effective for sorting elements uniformly distributed across a range and can perform well when the number of buckets is appropriately chosen relative to the number of elements. However, its performance can degrade if the distribution of elements is skewed or if there are too few buckets relative to the number of elements.\n\nOverall, bucket sort is a useful algorithm in scenarios where the input data is distributed evenly and the range of possible values is known and relatively small compared to the number of elements being sorted."
  ],
  "quickselect": [
    "Quickselect is an efficient algorithm for finding the k-th smallest (or largest) element in an unordered list. It works similarly to the quicksort algorithm by partitioning the array. Here's a common problem from Leetcode that can be solved using Quickselect:\n\nProblem: Kth Largest Element in an Array\n\nDescription:\nFind the k-th largest element in an unsorted array. Note that it is the k-th largest element in sorted order, not the k-th distinct element.\n\nExample 1:\nInput: [3,2,1,5,6,4] and k = 2\nOutput: 5\n\n\nExample 2:\nInput: [3,2,3,1,2,4,5,5,6] and k = 4\nOutput: 4\n\n\nNote:\nYou may assume k is always valid, 1 â‰¤ k â‰¤ array's length.\n\nSolution using Quickselect\n\npython\nimport random\n\nclass Solution:\n    def findKthLargest(self, nums: List[int], k: int) -> int:\n        def partition(left, right, pivot_index):\n            pivot = nums[pivot_index]\n            nums[pivot_index], nums[right] = nums[right], nums[pivot_index]\n            store_index = left\n             Move all smaller elements to the left\n            for i in range(left, right):\n                if nums[i] < pivot:\n                    nums[store_index], nums[i] = nums[i], nums[store_index]\n                    store_index += 1\n            nums[right], nums[store_index] = nums[store_index], nums[right]\n            return store_index\n        \n        def quickselect(left, right, k_smallest):\n            if left == right:\n                return nums[left]\n            \n            pivot_index = random.randint(left, right)\n            \n             Find the pivot position in a sorted list\n            pivot_index = partition(left, right, pivot_index)\n            \n             The pivot is in its final sorted position\n            if k_smallest == pivot_index:\n                return nums[k_smallest]\n            elif k_smallest < pivot_index:\n                return quickselect(left, pivot_index - 1, k_smallest)\n            else:\n                return quickselect(pivot_index + 1, right, k_smallest)\n        \n         kth largest is the (n - k)th smallest\n        return quickselect(0, len(nums) - 1, len(nums) - k)\n\n\nExplanation\n\n1. Partitioning:\n   - Similar to quicksort, the partition function rearranges the elements so that elements less than the pivot are on the left, elements greater than the pivot are on the right, and the pivot is in its final position.\n   \n2. Quickselect:\n   - This function recursively selects the k-th smallest element (which translates to the (n - k)-th smallest when finding the k-th largest element).\n   - It selects a random pivot and partitions the array.\n   - Depending on the pivot's position, it recursively searches in the left or right partition.\n\n3. Conversion:\n   - Since we need the k-th largest, we convert it to finding the (n - k)-th smallest element.\n\nKey Points\n\n- Time Complexity: Average O(n), Worst O(n^2)\n- Space Complexity: O(1) (in-place)\n\nUsing the median of medians ensures that the worst-case time complexity remains O(n), making it more robust for cases where consistent performance is critical.\n\nThis method is efficient for problems requiring the k-th largest or smallest element in an unsorted list and leverages the partitioning logic of quicksort to achieve optimal performance on average."
  ],
  "probability_and statistics": [
    "Probability and Statistics problem solving involves applying mathematical principles to analyze data and predict outcomes based on probability distributions. Here are a few key concepts and example problems that demonstrate how these principles are applied\n\nKey Concepts in Probability and Statistics\n\n1. Probability Basics:\n   - Probability of Events: Calculating the likelihood of an event occurring based on the total number of possible outcomes.\n   - Conditional Probability: Probability of an event given that another event has occurred.\n   - Bayes Theorem: Relates the conditional and marginal probabilities of two random events.\n\n2. Random Variables and Distributions:\n   - Discrete and Continuous Variables: Variables that can take on distinct values vs. those that can take on any value within a range.\n   - Probability Distributions: Describes the likelihood of different outcomes in a population.\n     - Binomial Distribution: Probability distribution of the number of successes in a fixed number of independent Bernoulli trials.\n     - Normal Distribution: Bell-shaped distribution that is symmetric around the mean.\n     - Poisson Distribution: Describes the number of events occurring in a fixed interval of time or space.\n   \n3. Statistical Measures:\n   - Mean, Median, Mode: Measures of central tendency.\n   - Variance and Standard Deviation: Measures of dispersion or spread of data points around the mean.\n\n4. Hypothesis Testing and Confidence Intervals:\n   - Hypothesis Testing: Assessing the validity of a hypothesis using statistical analysis.\n   - Confidence Intervals: Range of values within which a population parameter is estimated to lie.\n\nExample Problems\n\nHere are some example problems that illustrate different aspects of Probability and Statistics:\n\n1. Probability Problem:\n   - Calculate the probability of drawing a certain color from a bag of colored balls, given the total number of each color.\n   - Example: What is the probability of drawing a red ball from a bag containing 3 red, 2 blue, and 5 green balls?\n\n2. Distribution Problem:\n   - Determine the mean and standard deviation of a set of exam scores.\n   - Example: Given exam scores of 80, 85, 90, 75, and 95, calculate the mean and standard deviation.\n\n3. Hypothesis Testing Problem:\n   - Test whether the mean weight of apples produced by two different farms is significantly different.\n   - Example: Farm A claims their apples weigh, on average, 150 grams, while Farm B claims their apples weigh, on average, 160 grams. Test whether there is a significant difference in their average weights.\n\n4. Bayesian Inference Problem:\n   - Use Bayes theorem to update beliefs based on new evidence.\n   - Example: Given the results of a medical test, calculate the probability that a person has a disease, knowing the sensitivity and specificity of the test.\n\n5. Regression Problem:\n   - Predict the price of a house based on its size, location, and other factors using linear regression.\n   - Example: Develop a model to predict the selling price of houses based on features like square footage, number of bedrooms, and neighborhood.\n\nProblem-Solving Approach\n\nWhen approaching Probability and Statistics problems, it's essential to:\n- Understand the Problem Statement: Clearly define what needs to be calculated or inferred.\n- Choose the Right Approach: Select appropriate formulas, distributions, or statistical tests based on the problem requirements.\n- Calculate and Interpret Results: Perform calculations accurately and interpret the results in the context of the problem.\n- Validate and Test: Verify calculations and test assumptions to ensure accuracy and reliability of results."
  ],
  "doubly-linked_list": [
    "A doubly linked list is a type of linked list in which each node contains a data part and two pointers. The first pointer points to the next node in the sequence, and the second pointer points to the previous node. This structure allows traversal in both directions, making certain operations more efficient compared to a singly linked list.\n\nKey Features of a Doubly Linked List\n\n1. Node Structure:\n   - Each node contains three parts:\n     - Data: The value stored in the node.\n     - Next Pointer: A reference to the next node in the list.\n     - Prev Pointer: A reference to the previous node in the list.\n\n2. Head and Tail:\n   - The list has a head pointer that points to the first node and a tail pointer that points to the last node.\n\n3. Bidirectional Traversal:\n   - You can traverse the list from the head to the tail and from the tail to the head.\n\nCommon Operations\n\n1. Insertion:\n   - At the beginning: Insert a new node as the new head.\n   - At the end: Insert a new node as the new tail.\n   - In the middle: Insert a new node between two existing nodes.\n\n2. Deletion:\n   - From the beginning: Remove the head node.\n   - From the end: Remove the tail node.\n   - From the middle: Remove a node between two existing nodes.\n\n3. Traversal:\n   - You can traverse the list in both directions, forward (from head to tail) and backward (from tail to head).\n\nExample Use Case: LRU Cache\n\nThe LRU (Least Recently Used) Cache is a popular application of a doubly linked list combined with a hash map. Here's how it works:\n\n- Goal: Implement a cache that supports efficient `get` and `put` operations while maintaining the least recently used order.\n- Data Structures:\n  - Doubly Linked List: To maintain the order of usage.\n  - HashMap: To provide O(1) access to cache items by key.\n\nSteps for LRU Cache Implementation\n\n1. Initialize:\n   - Create a doubly linked list to keep track of the order of elements.\n   - Create a hash map to store references to the nodes in the list.\n\n2. Get Operation:\n   - Check if the key exists in the hash map.\n   - If it exists, move the corresponding node to the front of the list (indicating recent use).\n   - Return the value of the node.\n   - If the key does not exist, return -1.\n\n3. Put Operation:\n   - Check if the key already exists.\n   - If it exists, update the value and move the node to the front.\n   - If it does not exist, create a new node.\n   - Add the new node to the front of the list.\n   - Add the new node to the hash map.\n   - If the cache exceeds its capacity, remove the node from the end of the list (least recently used) and delete its reference from the hash map.\n\nExample Code: LRU Cache\n\nHere is a simplified implementation of an LRU Cache using a doubly linked list in Java:\n\nclass LRUCache {\n    class Node {\n        int key, value;\n        Node prev, next;\n        Node(int key, int value) {\n            this.key = key;\n            this.value = value;\n        }\n    }\n\n    private final int capacity;\n    private Map<Integer, Node> map;\n    private Node head, tail;\n\n    public LRUCache(int capacity) {\n        this.capacity = capacity;\n        this.map = new HashMap<>();\n        this.head = new Node(0, 0);\n        this.tail = new Node(0, 0);\n        head.next = tail;\n        tail.prev = head;\n    }\n\n    public int get(int key) {\n        if (map.containsKey(key)) {\n            Node node = map.get(key);\n            remove(node);\n            insert(node);\n            return node.value;\n        }\n        return -1;\n    }\n\n    public void put(int key, int value) {\n        if (map.containsKey(key)) {\n            remove(map.get(key));\n        }\n        if (map.size() == capacity) {\n            remove(tail.prev);\n        }\n        insert(new Node(key, value));\n    }\n\n    private void remove(Node node) {\n        map.remove(node.key);\n        node.prev.next = node.next;\n        node.next.prev = node.prev;\n    }\n\n    private void insert(Node node) {\n        map.put(node.key, node);\n        node.next = head.next;\n        node.prev = head;\n        head.next.prev = node;\n        head.next = node;\n    }\n}\n\n\nExplanation\n\n- Node Class: Represents each node in the doubly linked list.\n- LRUCache Class: Implements the LRU Cache using a doubly linked list and a hash map.\n  - Constructor: Initializes the cache with a given capacity and sets up the dummy head and tail nodes.\n  - Get Method: Retrieves a value from the cache. If the key exists, it moves the node to the front (most recently used) and returns its value.\n  - Put Method: Adds a new key-value pair to the cache. If the cache exceeds its capacity, it removes the least recently used item.\n  - Remove Method: Removes a node from the linked list.\n  - Insert Method: Inserts a node at the front of the linked list.\n\nThis structure allows for efficient O(1) time complexity for both `get` and `put` operations by leveraging the properties of the doubly linked list and hash map."
  ],
  "concurrency": [
    "Concurrency involves dealing with multiple tasks that are executing simultaneously and may interact with shared resources like variables or data structures. In programming challenges on LeetCode, concurrency problems typically require you to ensure that operations performed by multiple threads are synchronized and that shared resources are accessed safely to prevent issues like race conditions, data corruption, or inconsistent states.\n\nKey Concepts in Concurrency Problems:\n\n1. Threads: These are independent sequences of execution within a program. Each thread can execute code concurrently with other threads.\n\n2. Synchronization: This refers to techniques used to control access to shared resources to avoid conflicts. Common synchronization mechanisms include locks (like `synchronized` blocks in Java), mutexes, semaphores, and atomic operations.\n\n3. Race Conditions: These occur when the outcome of a program depends on the sequence or timing of uncontrollable events, like the execution order of threads. Race conditions can lead to unpredictable results and bugs.\n\n4. Atomicity: This refers to operations that are guaranteed to be executed as a single unit of work without interruption. In concurrent programming, atomic operations are crucial for ensuring data consistency.\n\n5. Thread Safety: This concept ensures that data structures and operations can be used safely in multithreaded environments without causing unexpected behavior or errors.\n\nExamples of Concurrency Problems on LeetCode:\n\n- Producer-Consumer Problem: Threads that produce data and threads that consume data from a shared buffer.\n\n- Readers-Writers Problem: Controlling access to a shared resource where multiple threads read and write data.\n\n- Thread Synchronization: Ensuring that threads execute in a specific order or synchronize their actions using locks or other mechanisms.\n\n- Concurrency in Algorithms: Problems where parallelism or concurrent execution can optimize performance, such as divide-and-conquer algorithms or parallel processing tasks.\n\nApproaches to Solve Concurrency Problems:\n\n- Locking: Use locks (e.g., `synchronized` blocks in Java, `Lock` interface) to synchronize access to shared resources.\n\n- Atomic Operations: Utilize atomic classes (`AtomicInteger`, `AtomicReference`, etc.) to ensure thread-safe operations on variables.\n\n- Thread Communication: Use mechanisms like `wait()`, `notify()`, `notifyAll()` in Java to coordinate actions between threads.\n\n- Concurrency Utilities: Leverage language-specific concurrency utilities (e.g., Java `ExecutorService`, `ThreadPoolExecutor`) for managing and executing tasks concurrently.\n\nImportance of Concurrency in LeetCode:\n\n- Concurrency problems on LeetCode assess your ability to implement thread-safe solutions and handle complex scenarios where multiple threads interact.\n  \n- Understanding concurrency concepts and applying them correctly is crucial for writing efficient, scalable, and correct multithreaded programs.\n\nBy mastering concurrency concepts and practicing with problems on LeetCode, you can develop robust solutions that handle simultaneous execution gracefully and efficiently."
  ],
  "iterator": [
    "How to Approach LeetCode Problems Using Iterators\n\nWhen dealing with LeetCode problems that require the use of iterators, it typically involves traversing and manipulating data structures such as arrays, linked lists, trees, or graphs. Hereâ€™s a structured approach to tackle such problems effectively:\n\n1. Understand the Problem Requirements:\n   - Read the problem statement carefully to grasp what data structure you need to iterate over (e.g., array, linked list, tree).\n   - Identify specific operations or transformations you need to perform on the data elements.\n\n2. Choose the Right Iterator:\n   - Select an appropriate iterator based on the data structure:\n     - Array/List Iterator: Use indices or built-in iterators (`Iterator` in Java, `enumerate` in Python).\n     - Linked List Iterator: Implement a custom iterator or use Java's `LinkedListIterator`.\n     - Tree Iterator: Implement an in-order, pre-order, or post-order traversal iterator.\n     - Graph Iterator: Implement BFS or DFS using a queue or stack-based iterator.\n\n3. Implement the Iterator:\n   - Depending on the chosen data structure and required operations, implement or instantiate the iterator.\n   - Ensure the iterator adheres to the requirements of the problem statement, such as order of traversal, handling null or empty structures, etc.\n\n4. Iterate and Perform Operations:\n   - Use the iterator to traverse through the data structure while performing necessary operations.\n   - Handle edge cases such as empty structures, bounds checking, or null elements as required.\n\n5. Optimize and Analyze Complexity:\n   - Analyze the time and space complexity of your solution with respect to iterator usage.\n   - Optimize your solution if possible by reducing unnecessary iterations or improving data structure access patterns.\n\n6. Test and Validate:\n   - Test your solution with various test cases, including edge cases provided in the problem statement.\n   - Verify correctness and efficiency through LeetCodeâ€™s built-in test cases or custom test cases.\n\nExample Scenario\n\nSuppose you have a problem where you need to iterate through a linked list of integers and perform specific operations. Hereâ€™s a simplified example approach:\n\nProblem: Given a singly linked list, reverse every k nodes. If the number of nodes is not a multiple of k, leave the remaining nodes as is.\n\nApproach:\n- Implement an iterator to traverse through the linked list.\n- Use a helper method to reverse nodes in groups of k.\n- Handle the remaining nodes if the count is not a multiple of k.\n\nImplementation:\n- Instantiate a custom iterator for the linked list.\n- Use the iterator to traverse through nodes in groups of k.\n- Reverse the nodes in each group using an iterative approach.\n- Adjust pointers accordingly and handle remaining nodes if less than k.\n\nComplexity Analysis:\n- Time Complexity: O(n), where n is the number of nodes in the linked list.\n- Space Complexity: O(1), as the reversal is done in place without additional space.",
    "Example: Iterating through an Array Using Iterator in Java\n\nimport java.util.*;\n\npublic class IteratorExample {\n\n    public static void main(String[] args) {\n        int[] nums = {1, 2, 3, 4, 5};        \n        Iterator<Integer> iterator = Arrays.stream(nums).iterator();\n        \n        int sumOfSquares = 0;\n        \n        while (iterator.hasNext()) {\n            int num = iterator.next();\n            int square = num * num;\n            sumOfSquares += square;\n        }\n        \n        System.out.println(\"Sum of squares: \" + sumOfSquares);\n    }\n}\n\nExplanation:\n\n1. Initialization: \n   - We have an array `nums` containing integers `{1, 2, 3, 4, 5}`.\n   - We convert the array to an `IntStream` using `Arrays.stream(nums)` and then obtain an iterator using `iterator()` method.\n\n2. Iterating with Iterator: \n   - We initialize `sumOfSquares` to accumulate the sum of squares.\n   - Using a `while` loop, we iterate through each element in the array using `iterator.hasNext()` to check if there are more elements.\n   - `iterator.next()` retrieves the next element in the array.\n   - We square the element (`num * num`) and add it to `sumOfSquares`.\n\n3. Output:\n   - Finally, we print out the `sumOfSquares`, which is the result of summing up all the squared values of the array elements.\n\nNotes:\n- In this example, `Iterator<Integer>` is used to iterate through the array elements. It provides a simple and efficient way to traverse collections or arrays.\n- You can adapt this approach to various LeetCode problems that require iterating through arrays or other data structures and performing specific operations efficiently.\n- Make sure to handle any special cases or edge cases depending on the problem requirements (e.g., empty arrays, null elements)."
  ],
  "merge_sort": [
    "Merge Sort is a classic sorting algorithm that follows the divide-and-conquer paradigm. It works by dividing the array into smaller subarrays, sorting those subarrays, and then merging them back together to form a sorted array.\n\nSteps of Merge Sort:\n\n1. Divide: Split the array into two halves.\n2. Conquer: Recursively sort both halves.\n3. Combine: Merge the two sorted halves back together into one sorted array.\n\nHere's a high-level example:\n\n1. Given an array `[38, 27, 43, 3, 9, 82, 10]`.\n2. Split it into two halves: `[38, 27, 43, 3]` and `[9, 82, 10]`.\n3. Recursively split those halves until you get subarrays with one element each: `[38]`, `[27]`, `[43]`, `[3]`, `[9]`, `[82]`, `[10]`.\n4. Merge the subarrays back together: `[27, 38]`, `[3, 43]`, `[9, 10, 82]`.\n5. Continue merging until you have the fully sorted array: `[3, 9, 10, 27, 38, 43, 82]`.\n\nMerge Sort Implementation in Java:\n\npublic class MergeSort {\n    public void mergeSort(int[] array) {\n        if (array.length > 1) {\n            int mid = array.length / 2;\n            \n            int[] left = new int[mid];\n            int[] right = new int[array.length - mid];\n            \n            System.arraycopy(array, 0, left, 0, mid);\n            System.arraycopy(array, mid, right, 0, array.length - mid);\n            \n            mergeSort(left);\n            mergeSort(right);\n            \n            // Merge sorted halves\n            merge(array, left, right);\n        }\n    }\n\n    private void merge(int[] result, int[] left, int[] right) {\n        int i = 0, j = 0, k = 0;\n        \n        while (i < left.length && j < right.length) {\n            if (left[i] <= right[j]) {\n                result[k++] = left[i++];\n            } else {\n                result[k++] = right[j++];\n            }\n        }\n        \n        while (i < left.length) {\n            result[k++] = left[i++];\n        }\n        \n        while (j < right.length) {\n            result[k++] = right[j++];\n        }\n    }\n}\n\nInternal and External Sorting\n\n- Internal Sorting: Sorting is performed entirely in main memory (RAM). Merge Sort is often used as an internal sorting algorithm because it has good performance characteristics.\n- External Sorting: Sorting is performed with data stored in external memory (like disk storage). Merge Sort is particularly well-suited for external sorting due to its divide-and-conquer approach which allows it to handle large datasets that do not fit entirely into memory.\n\nTime and Space Complexities\n\n- Time Complexity: \n  - Best Case: (O(n log n))\n  - Average Case: (O(n log n))\n  - Worst Case: (O(n log n))\n- Space Complexity: (O(n)) due to the auxiliary arrays used for merging.\n\nWhen to Use Merge Sort\n\n- Stable Sort Requirement: Merge Sort is stable, meaning it preserves the relative order of equal elements.\n- Large Data Sets: When dealing with large data sets that do not fit into memory, Merge Sort can be adapted for external sorting.\n- Linked Lists: Merge Sort works well with linked lists since it doesn't require random access to elements, unlike quicksort.\n- Consistent Performance: If worst-case performance is a concern, Merge Sort guarantees (O(n log n)) time complexity, whereas other algorithms like quicksort can degrade to (O(n^2)) in the worst case.\n\nIn summary, Merge Sort is a reliable and efficient sorting algorithm suitable for various scenarios, particularly when stability and consistent performance are crucial.",
    "Sure! To perform merge sort using two different arrays, one for the original data and another for the auxiliary storage, you can follow a similar approach but ensure the operations are done using the two arrays. This approach will help in understanding the mechanics of merge sort better.\n\nHereâ€™s a detailed explanation along with Java code:\n\n Explanation\n\n1. Divide: The main array is divided into two halves.\n2. Conquer: Each half is recursively sorted.\n3. Combine: Two halves are merged back into a single sorted array using an auxiliary array.\n\nJava Implementation\n\nHere, we use two arrays: `arr` for storing the data and `aux` as an auxiliary array for merging purposes.\n\npublic class MergeSortTwoArrays {\n    public void mergeSort(int[] arr) {\n        int[] aux = new int[arr.length];\n        mergeSort(arr, aux, 0, arr.length - 1);\n    }\n\n    private void mergeSort(int[] arr, int[] aux, int low, int high) {\n        if (low < high) {\n            int mid = low + (high - low) / 2;\n\n            mergeSort(arr, aux, low, mid);\n            mergeSort(arr, aux, mid + 1, high);\n            merge(arr, aux, low, mid, high);\n        }\n    }\n\n    private void merge(int[] arr, int[] aux, int low, int mid, int high) {\n        for (int k = low; k <= high; k++) {\n            aux[k] = arr[k];\n        }\n\n        int i = low, j = mid + 1;\n\n        for (int k = low; k <= high; k++) {\n            if (i > mid) {\n                arr[k] = aux[j++];\n            } else if (j > high) {\n                arr[k] = aux[i++];\n            } else if (aux[j] < aux[i]) {\n                arr[k] = aux[j++];\n            } else {\n                arr[k] = aux[i++];\n            }\n        }\n    }\n\n    public static void main(String[] args) {\n        int[] arr = {38, 27, 43, 3, 9, 82, 10};\n        MergeSortTwoArrays sorter = new MergeSortTwoArrays();\n        sorter.mergeSort(arr);\n        System.out.println(Arrays.toString(arr));\n    }\n}\n\n Internal and External Sorting\n\n- Internal Sorting: Sorting is done within the main memory.\n  - The implementation above is an example of internal sorting where all the data is assumed to fit into the main memory.\n\n- External Sorting: Sorting is done when the data does not fit into the main memory.\n  - Typically, external sorting involves breaking down the data into manageable chunks that fit into the main memory, sorting those chunks, and then merging the sorted chunks. This is often used with merge sort in a multi-pass approach where sorted runs (chunks) are merged together iteratively.\n\n Time and Space Complexities\n\n- Time Complexity:\n  - Best Case: (O(n log n))\n  - Average Case: (O(n log n))\n  - Worst Case: (O(n log n))\n\n- Space Complexity: (O(n)) due to the auxiliary array used for merging.\n\n When to Use Merge Sort\n\n- Stable Sort Requirement: Merge sort maintains the relative order of equal elements.\n- Large Data Sets: Suitable for external sorting with large data sets that exceed memory limits.\n- Consistent Performance: Guarantees (O(n log n)) performance in the worst-case scenario.\n- Linked Lists: Naturally suited for linked lists as it does not require random access to elements.\n\nIn conclusion, merge sort using two arrays involves dividing the data, recursively sorting each part, and merging the sorted parts using an auxiliary array. This method provides a clear understanding of the merge sort process and is efficient for various use cases."
  ],
  "randomized": [
    "Randomized problems on LeetCode typically involve using randomization techniques to solve problems efficiently or to design data structures that can handle random operations in constant or near-constant time. Here are some of the most common and interesting problems involving randomization on LeetCode, along with a brief explanation and the approach for solving them:\n\n1. Insert Delete GetRandom O(1) (LeetCode Problem #380)\n\nProblem Description:\nDesign a data structure that supports all following operations in average O(1) time:\n- `insert(val)`: Inserts an item val to the set if not already present.\n- `remove(val)`: Removes an item val from the set if present.\n- `getRandom()`: Returns a random element from the current set of elements. Each element must have the same probability of being returned.\n\nSolution Approach:\nUse a combination of a list and a dictionary:\n- The list allows for O(1) time access to elements.\n- The dictionary maps values to their indices in the list, allowing for O(1) time insertions and deletions.\n\nimport random\n\nclass RandomizedSet:\n    def __init__(self):\n        self.nums = []\n        self.pos = {}\n\n    def insert(self, val: int) -> bool:\n        if val in self.pos:\n            return False\n        self.pos[val] = len(self.nums)\n        self.nums.append(val)\n        return True\n\n    def remove(self, val: int) -> bool:\n        if val not in self.pos:\n            return False\n        idx, last = self.pos[val], self.nums[-1]\n        self.nums[idx], self.pos[last] = last, idx\n        self.nums.pop()\n        del self.pos[val]\n        return True\n\n    def getRandom(self) -> int:\n        return random.choice(self.nums)\n\n2. Random Pick with Weight (LeetCode Problem #528)\n\nProblem Description:\nGiven an array of positive integers w where w[i] describes the weight of index i, write a function `pickIndex` which randomly picks an index in proportion to its weight.\n\nSolution Approach:\nUse a prefix sum array and binary search:\n- Compute the prefix sum of the weights.\n- Use `random.random()` to pick a random value and find its corresponding index using binary search.\n\nimport random\nimport itertools\n\nclass Solution:\n    def __init__(self, w: List[int]):\n        self.prefix_sums = list(itertools.accumulate(w))\n        self.total_sum = self.prefix_sums[-1]\n\n    def pickIndex(self) -> int:\n        target = random.random() * self.total_sum\n        return bisect.bisect_left(self.prefix_sums, target)\n\n3. Shuffle an Array (LeetCode Problem #384)\n\nProblem Description:\nGiven an integer array nums, design an algorithm to shuffle the array randomly. Implement the `Solution` class:\n- `Solution(int[] nums)` Initializes the object with the integer array nums.\n- `int[] reset()` Resets the array to its original configuration and returns it.\n- `int[] shuffle()` Returns a random shuffling of the array.\n\nSolution Approach:\nUse the Fisher-Yates shuffle algorithm:\n- Iterate over the array and for each position, swap it with a randomly chosen element that comes after or the element itself.\n\nimport random\n\nclass Solution:\n    def __init__(self, nums: List[int]):\n        self.original = nums[:]\n        self.array = nums[:]\n\n    def reset(self) -> List[int]:\n        self.array = self.original[:]\n        return self.array\n\n    def shuffle(self) -> List[int]:\n        for i in range(len(self.array)):\n            swap_idx = random.randint(i, len(self.array) - 1)\n            self.array[i], self.array[swap_idx] = self.array[swap_idx], self.array[i]\n        return self.array\n\n4. Random Pick Index (LeetCode Problem #398)\n\nProblem Description:\nGiven an array of integers with possible duplicates, randomly output the index of a given target number. Implement the `Solution` class:\n- `Solution(int[] nums)` Initializes the object with the array nums.\n- `int pick(int target)` Picks a random index i such that `nums[i] == target`.\n\nSolution Approach:\nUse Reservoir Sampling:\n- Iterate through the array, and for each occurrence of the target, decide whether to pick this index or not with probability 1/count.\n\nimport random\n\nclass Solution:\n    def __init__(self, nums: List[int]):\n        self.nums = nums\n\n    def pick(self, target: int) -> int:\n        count = 0\n        result = None\n        for i, num in enumerate(self.nums):\n            if num == target:\n                count += 1\n                if random.randint(1, count) == 1:\n                    result = i\n        return result\n\nThese problems cover various aspects of using randomization for efficient data structure operations, probability-based selections, and shuffling techniques. Each of these problems demonstrates how to handle randomness in different contexts effectively."
  ],
  "monotonic_queue": [
    "Monotonic Queue is a data structure that maintains elements in a way that supports efficient querying of either the minimum or maximum value in the current set of elements, depending on whether it's a monotonic increasing or decreasing queue. This structure is useful for problems that require sliding window operations or maintaining order properties dynamically.\n\nExample Problem: Sliding Window Maximum\n\nLet's illustrate the use of Monotonic Queue with an example problem from LeetCode: 239. Sliding Window Maximum. This problem asks to find the maximum value in each sliding window of size `k` over a given array.\n\n#Problem Statement:\nGiven an array `nums`, there is a sliding window of size `k` which is moving from the leftmost to the rightmost position in the array. You need to find the maximum value in each sliding window.\n\nExample:\nplaintext\nInput: nums = [1,3,-1,-3,5,3,6,7], k = 3\nOutput: [3,3,5,5,6,7]\nExplanation: \n\nWindow position                Max\n---------------               -----\n[1  3  -1] -3  5  3  6  7       3\n 1 [3  -1  -3] 5  3  6  7       3\n 1  3 [-1  -3  5] 3  6  7       5\n 1  3  -1 [-3  5  3] 6  7       5\n 1  3  -1  -3 [5  3  6] 7       6\n 1  3  -1  -3  5 [3  6  7]      7\n\n\nSolution Using Monotonic Queue\n\nHere's how we can solve the problem using a Monotonic Queue:\n\n1. Initialization: Use a deque (double-ended queue) to store indices of array elements.\n2. Sliding Window Process:\n   - For each element in the array:\n     - Remove indices from the front of the deque that are out of the current window (`i >= k`).\n     - Maintain the deque such that elements in it are in decreasing order of their values in `nums`.\n     - Add current index `i` to the deque while ensuring that any smaller elements before it are removed (monotonic decreasing order).\n     - Add the maximum element (from the front of the deque) to the result when the window size `i - k + 1` is greater than or equal to `k`.\n\nHere's the Java implementation using a Monotonic Queue for the above problem:\n\nimport java.util.*;\n\nclass Solution {\n    public int[] maxSlidingWindow(int[] nums, int k) {\n        int n = nums.length;\n        if (n * k == 0) return new int[0];\n        if (k == 1) return nums;\n        \n        Deque<Integer> deque = new LinkedList<>();\n        int[] result = new int[n - k + 1];\n        \n        for (int i = 0; i < n; i++) {\n            while (!deque.isEmpty() && deque.peek() < i - k + 1) {\n                deque.poll();\n            }\n            \n            while (!deque.isEmpty() && nums[deque.peekLast()] < nums[i]) {\n                deque.pollLast();\n            }\n            \n            deque.offer(i);\n            \n            if (i - k + 1 >= 0) {\n                result[i - k + 1] = nums[deque.peek()];\n            }\n        }\n        \n        return result;\n    }\n}\n\n\nExplanation of the Code:\n\n- Deque Usage: \n  - `deque` is used to store indices of array elements.\n  - Elements are added and removed from both ends of the deque to maintain the required order.\n- Sliding Window Process:\n  - Iterate through the array `nums`.\n  - Maintain the window size `k`.\n  - Adjust the deque by removing indices that are out of the current window (`i - k + 1`).\n  - Ensure the deque is in a decreasing order of element values.\n  - Add the maximum element from the deque to the `result` array for each valid window position.\n\nThis approach ensures that the maximum value for each sliding window of size `k` is efficiently computed using the properties of the Monotonic Queue, providing an optimal solution to the problem.",
    "Let's compare a monotonic queue, a standard queue, and a priority queue to understand their differences, use cases, and characteristics.\n\nMonotonic Queue\n\nA monotonic queue is a specialized data structure that maintains its elements in a sorted order (either non-increasing or non-decreasing). It is particularly useful for problems involving sliding windows, where you need to efficiently find the minimum or maximum value in a moving window.\n\nCharacteristics:\n- Order: Elements are kept in a specific order (increasing or decreasing).\n- Operations:\n  - Enqueue: Adds an element while maintaining the order.\n  - Dequeue: Removes elements from the front.\n  - Maintain Monotonicity: Removes elements from the back/front to ensure the monotonic property.\n- Use Cases:\n  - Finding minimum or maximum in a sliding window.\n  - Problems requiring efficient range queries within a window.\n\nExample:\nIn the problem of finding the shortest subarray with a sum at least ( K ), a monotonic queue helps maintain indices of the prefix sum array in increasing order, facilitating efficient subarray sum calculations.\n\nStandard Queue\n\nA standard queue is a basic data structure that follows the First-In-First-Out (FIFO) principle. It allows elements to be added to the end and removed from the front.\n\nCharacteristics:\n- Order: Maintains the order of insertion.\n- Operations:\n  - Enqueue: Adds an element to the end.\n  - Dequeue: Removes an element from the front.\n- Use Cases:\n  - Breadth-First Search (BFS) in graphs.\n  - Task scheduling and order processing.\n  - Managing buffers in computer systems.\n\nExample:\nA queue can be used to implement BFS, where nodes are processed in the order they are discovered.\n\nPriority Queue\n\nA priority queue is a data structure that allows elements to be removed based on priority rather than order of insertion. Each element has a priority, and the element with the highest (or lowest) priority is dequeued first.\n\nCharacteristics:\n- Order: Maintains elements based on their priority.\n- Operations:\n  - Enqueue: Adds an element with a priority.\n  - Dequeue: Removes the element with the highest (or lowest) priority.\n- Use Cases:\n  - Dijkstraâ€™s algorithm for shortest paths.\n  - Task scheduling where tasks have different priorities.\n  - Event-driven simulation systems.\n\nExample:\nIn Dijkstra's algorithm, a priority queue is used to efficiently select the next node with the smallest tentative distance.\n\nComparison\n\n| Feature           | Monotonic Queue                            | Standard Queue                    | Priority Queue                         |\n|-------------------|--------------------------------------------|-----------------------------------|----------------------------------------|\n| Order         | Maintains a specific monotonic order       | FIFO order                        | Based on element priority              |\n| Complexity    | O(1) amortized per operation               | O(1) per operation                | O(log n) for insertion and deletion    |\n| Use Case      | Sliding window minimum/maximum             | BFS, task scheduling              | Dijkstraâ€™s algorithm, priority-based tasks |\n| Data Structure| Deque                                      | Queue (LinkedList, ArrayDeque)    | Binary heap, Fibonacci heap            |\n\nConclusion\n\n- Monotonic Queue is ideal for problems that involve maintaining a sorted order within a sliding window for efficient minimum/maximum queries.\n- Standard Queue is useful for general FIFO operations like BFS and task processing.\n- Priority Queue is suited for tasks where elements need to be processed based on priority, such as in shortest path algorithms or scheduling tasks with different priorities."
  ]
}